---
title: "Assignment 7"
author: "Charles Lang"
date: "12/1/2016"
output: html_document
---
#Create Data
```{r}
id <- seq(1,1000,1)
D1 <- data.frame(id)

post.test1 <- rnorm(500, 0.4,0.07)
post.test1 <- ifelse(post.test1 < 0, 0, post.test1)
post.test1 <- ifelse(post.test1 > 1, 1, post.test1)
post.test2 <- rnorm(500, 0.6,0.07)
post.test2 <- ifelse(post.test2 < 0, 0, post.test2)
post.test2 <- ifelse(post.test2 > 1, 1, post.test2)
#post.test3 <- rnorm(300, 0.75,0.07)
#post.test3 <- ifelse(post.test3 > 1, 1, post.test3)
D1$post.test.score <- round(c(post.test1,post.test2),2)

D1$pre.test.score <- round(c(post.test1,post.test2), 2)

D1$messages <- round(sample(c(150:200),1000, replace = TRUE)*D1$post.test.score)
D1$forum.posts <- round(sample(c(0:40),1000, replace = TRUE)*(D1$post.test.score) + 2)
D1$av.assignment.score <- round(D1$post.test.score*rnorm(1000, 0.5,0.1),2)
D1$level.up <- ifelse(D1$post.test.score > 0.45 & D1$av.assignment.score > 0.25, "yes", "no")

```

#Visualize the distributions
```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
#HINT: look up "facet" in the ggplot documentation
D2 <- select(D1, 1:7)
#Convert yes/no to 1/0 to avoid mixing variable types
D2$level.up <- ifelse(D2$level.up == "yes", 1,0)
D3 <- gather(D2, "measure", "score", 2:7)

p1 <- ggplot(D3, aes(score)) + facet_wrap(~measure, scales = "free")
p1 + geom_histogram(stat = "count")

#Visualize the relationships between variables
pairs(D2)
```

#Classification tree
```{r}
#Create a classification tree that predicts whether a student "levels up" in the online course using three variables of your choice (remember to set all controls to their minimums)

library(rpart)
c.tree1 <- rpart(level.up ~ forum.posts + pre.test.score, method = "class", data = D1, control=rpart.control(minsplit=1, minbucket=1, cp=0.001))

printcp(c.tree1)
plot(c.tree1)
text(c.tree1)

#Generate a probability value that represents the probability that a student levels up based your classification tree 

D1$pred <- predict(c.tree1, type = "prob")[,2]#Last class we used type = "class" which predicted the classification for us, this time we are using type = "prob" to see the probability that our previous classififcation was based on.

#Now you can generate the ROC curve for your model. You will need to install the package ROCR to do this.

library(ROCR)

#Plot the curve
pred.detail <- prediction(D1$pred, D1$level.up) 
plot(performance(pred.detail, "tpr", "fpr"))
abline(0, 1, lty = 2)

#Calculate the Area Under the Curve
unlist(slot(performance(pred.detail,"auc"), "y.values"))#unlist liberates AUC value from the "performance" object created by ROCR

#Now repeat this process, but using the variables you did not use for the previous model and compare the plots & results of your two models. Which one do you think was the better model? 

```
#Thresholds
```{r}
#Look at the ROC plot for your first model. Based on this plot choose a probability threshold that balances capturing the most correct predictions against false positives. Then generate a new variable in your data set that classifies each student according to your chosen threshold.

D1$threshold.pred1 <- ifelse(D1$pred >= 0.8, "yes", "no")

D1$threshold.pred2 <- ifelse(D1$pred >= 0.95, "yes", "no")

D1$threshold.pred3 <- ifelse(D1$pred >= 0.25, "yes", "no")

#Now generate three diagnostics:

accuracy.model1 <- mean(ifelse(D1$level.up == D1$threshold.pred1, 1, 0))

D1$truepos.model1 <- ifelse(D1$level.up == "yes" & D1$threshold.pred1 == "yes", 1, 0)
D1$falsepos.model1 <- ifelse(D1$level.up == "no" & D1$threshold.pred1 == "yes", 1,0)
D1$falseneg.model1 <- ifelse(D1$level.up == "yes" & D1$threshold.pred1 == "no", 1,0)

precision.model1 <- sum(D1$truepos.model1)/(sum(D1$truepos.model1) + sum(D1$falsepos.model1))

recall.model1 <- sum(D1$truepos.model1)/(sum(D1$truepos.model1) + sum(D1$falseneg.model1))

#Finally, calculate Kappa for your model according to:

#First generate the table of comparisons
table1 <- table(D1$level.up, D1$threshold.pred1)
table1

#Calculate kappa manually
po <- (586+247)/(586+247+14+153)
pe <- ((586 + 14)/(586+247+14+153))*((586 + 153)/(586+247+14+153)) + ((14 + 247)/(586+247+14+153))*((153+247)/(586+247+14+153))

kappa <- (po - pe)/(1 - pe)

#Calculate OOB
library(psych) #You could also use the "irr" or "vcd" library versions
cohen.kappa(table1)

#Now choose a different threshold value and repeat these diagnostics. What conclusions can you draw about your two thresholds?

#Alternate kappa value

library(irr)

kappa2(D1[,c(7,9)], "unweighted")

kappa2(D1[,c(7,10)], "unweighted")

kappa2(D1[,c(7,11)], "unweighted")

```


